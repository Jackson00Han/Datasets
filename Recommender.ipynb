{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPannokCByOkoY8qUjeJMEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jackson00Han/Datasets/blob/master/Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User-User Matching Algorithm"
      ],
      "metadata": {
        "id": "9Z7o1BI9k6px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. basic similarity score calculation algorithm\n",
        "2. content-based filtering algorithm\n",
        "3. collaborative filtering algorithm\n",
        "4. hybrid system"
      ],
      "metadata": {
        "id": "pzNz78INsx4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_EqooDhSkYYl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "AXvMKzACvq9w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set seed for reproducibility (so results are consistent when generating random values)\n",
        "np.random.seed(1)\n",
        "\n",
        "# Step 1: Generate synthetic user data\n",
        "\n",
        "num_samples = 100  # Define the number of user profiles\n",
        "\n",
        "# Generate random data for each column (attributes of each user)\n",
        "names = [f\"User_{i}\" for i in range(1, num_samples + 1)]  # User names\n",
        "ages = np.random.randint(18, 65, size=num_samples)  # Ages between 18 and 64\n",
        "nationalities = np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France', 'India', 'China'], size=num_samples)\n",
        "languages = np.random.choice(['English', 'French', 'German', 'Spanish', 'Hindi', 'Chinese'], size=num_samples)\n",
        "residence_countries = np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France', 'India', 'China'], size=num_samples)\n",
        "postal_codes = np.random.randint(10000, 99999, size=num_samples)  # Random postal codes\n",
        "occupations = np.random.choice(['Engineer', 'Artist', 'Doctor', 'Lawyer', 'Teacher', 'Entrepreneur'], size=num_samples)\n",
        "marital_statuses = np.random.choice(['Single', 'Married', 'Divorced'], size=num_samples)\n",
        "books = np.random.choice(['Fiction', 'Non-fiction', 'Sci-Fi', 'Fantasy', 'Biography', 'History'], size=num_samples)\n",
        "music = np.random.choice(['Rock', 'Jazz', 'Classical', 'Pop', 'Hip-hop', 'Country'], size=num_samples)\n",
        "activity_levels = np.random.randint(1, 11, size=num_samples)  # Activity levels between 1 and 10\n",
        "mana_levels = np.random.randint(0, 101, size=num_samples)  # MANA levels between 0 and 100\n",
        "\n",
        "# Step 2: Create a DataFrame to store the user data\n",
        "\n",
        "user_data = pd.DataFrame({\n",
        "    'Name': names,\n",
        "    'Age': ages,\n",
        "    'Nationality': nationalities,\n",
        "    'Language': languages,\n",
        "    'Residence Country': residence_countries,\n",
        "    'Postal Code': postal_codes,\n",
        "    'Occupation': occupations,\n",
        "    'Marital Status': marital_statuses,\n",
        "    'Favorite Book Genre': books,\n",
        "    'Favorite Music Genre': music,\n",
        "    'Activity Level': activity_levels,\n",
        "    'MANA': mana_levels  # Adding the 'MANA' column, which is a numeric feature\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Preprocess and encode categorical features\n",
        "\n",
        "# List of categorical columns to encode\n",
        "categorical_columns = ['Nationality', 'Language', 'Residence Country', 'Occupation', 'Marital Status',\n",
        "                       'Favorite Book Genre', 'Favorite Music Genre']\n",
        "\n",
        "# Initialize LabelEncoders to convert categorical values into numeric labels\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    user_data[col + '_encoded'] = label_encoders[col].fit_transform(user_data[col])\n",
        "\n",
        "# Step 4: Prepare features for similarity calculation\n",
        "\n",
        "# Numeric features for Euclidean distance\n",
        "numeric_features = ['Age', 'Activity Level', 'MANA']\n",
        "\n",
        "# Encoded categorical features for Cosine and Jaccard similarities\n",
        "categorical_features = [col + '_encoded' for col in categorical_columns]\n",
        "\n",
        "# Step 5: Define similarity functions\n",
        "\n",
        "# Function to calculate Euclidean similarity for numeric features\n",
        "def euclidean_similarity(user1_data, user2_data, features):\n",
        "    # Euclidean distance is converted to similarity by using 1 / (1 + distance)\n",
        "    return 1 / (1 + euclidean(user1_data[features], user2_data[features]))\n",
        "\n",
        "# Function to calculate Cosine similarity for categorical features\n",
        "def cosine_similarity_features(user1_data, user2_data, features):\n",
        "    # Cosine similarity compares the angles between vectors of encoded features\n",
        "    return cosine_similarity([user1_data[features]], [user2_data[features]])[0][0]\n",
        "\n",
        "# One-hot encode categorical features for Jaccard similarity\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded = onehot_encoder.fit_transform(user_data[categorical_features])\n",
        "\n",
        "# Function to calculate Jaccard similarity using one-hot encoded data\n",
        "def jaccard_similarity_onehot(user1_idx, user2_idx, onehot_encoded_data):\n",
        "    # Compare one-hot encoded vectors to measure set similarity\n",
        "    user1 = onehot_encoded_data[user1_idx]\n",
        "    user2 = onehot_encoded_data[user2_idx]\n",
        "    intersection = np.sum(np.minimum(user1, user2))\n",
        "    union = np.sum(np.maximum(user1, user2))\n",
        "    return intersection / union if union != 0 else 0\n",
        "\n",
        "# Step 6: Define a function to calculate the overall similarity\n",
        "\n",
        "def calculate_similarity(user1_idx, user2_idx, user_data, onehot_encoded_data):\n",
        "    # Extract the data for the two users being compared\n",
        "    user1_data = user_data.iloc[user1_idx]\n",
        "    user2_data = user_data.iloc[user2_idx]\n",
        "\n",
        "    # Compute Euclidean similarity on numeric features\n",
        "    euclidean_sim = euclidean_similarity(user1_data, user2_data, numeric_features)\n",
        "\n",
        "    # Compute Cosine similarity on encoded categorical features\n",
        "    cosine_sim = cosine_similarity_features(user1_data, user2_data, categorical_features)\n",
        "\n",
        "    # Compute Jaccard similarity on one-hot encoded categorical data\n",
        "    jaccard_sim = jaccard_similarity_onehot(user1_idx, user2_idx, onehot_encoded_data)\n",
        "\n",
        "    # Combine the three similarity measures into an overall similarity score\n",
        "    overall_similarity = np.mean([euclidean_sim, cosine_sim, jaccard_sim])\n",
        "\n",
        "    return overall_similarity\n",
        "\n",
        "# Step 7: Function to compute similarities and sort them in descending order\n",
        "\n",
        "def find_most_similar_users(target_user_idx, user_data, onehot_encoded_data, numeric_features, categorical_features,top_n):\n",
        "    similarities = []\n",
        "\n",
        "    # Loop through all users and calculate the similarity with the target user\n",
        "    for user_idx in range(len(user_data)):\n",
        "        if user_idx != target_user_idx:  # Skip the target user itself\n",
        "            sim_score = calculate_similarity(target_user_idx, user_idx, user_data, onehot_encoded_data)  # Call the similarity function\n",
        "            similarities.append((user_idx, sim_score))  # Store user index and similarity score\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_similarities[:top_n]\n",
        "\n",
        "def recommend_for_all_users_dataframe(user_data, onehot_encoded_data, numeric_features, categorical_features, top_n):\n",
        "    # Create a list to hold results for each user\n",
        "    all_recommendations = []\n",
        "\n",
        "    # Loop through each user and get the top N similar users for that user\n",
        "    for user_idx in range(len(user_data)):\n",
        "        top_similar_users = find_most_similar_users(user_idx, user_data, onehot_encoded_data, numeric_features, categorical_features, top_n)\n",
        "        # Extract only the user indices (first element of each tuple) from the list of tuples\n",
        "        top_similar_user_indices = [user[0] for user in top_similar_users]\n",
        "        # Append the target user and their top N similar users as a row\n",
        "        all_recommendations.append([user_idx] + top_similar_user_indices)\n",
        "\n",
        "    # Create column names for the DataFrame\n",
        "    column_names = ['User'] + [f'Top_{i+1}_Similar_User' for i in range(top_n)]\n",
        "\n",
        "    # Convert the list of recommendations into a pandas DataFrame\n",
        "    recommendations_df = pd.DataFrame(all_recommendations, columns=column_names)\n",
        "\n",
        "    return recommendations_df\n",
        "\n",
        "# Example: Find the top 3 most similar users for all users and display in a DataFrame\n",
        "recommendations_df = recommend_for_all_users_dataframe(user_data, onehot_encoded, numeric_features, categorical_features, top_n=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1cdc8tyBTpT",
        "outputId": "a440832f-162d-4ab4-b78b-a26ed6f12014"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's simulate a scenario where, after using a simple algorithm to calculate user similarity and recommend the top 10 people for each user, feedback is collected a few days later. Based on the feedback, each user shows interest in 0 to 2 other users. The interest is generated by assuming it is distributed as follows:\n",
        "\n",
        "90% of the users they are interested in come from the list of 10 recommended users.\n",
        "\n",
        "10% of the users they are interested in come from other users outside the recommended list."
      ],
      "metadata": {
        "id": "duZoXzkVjn8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Phase for further matching\n",
        "df = user_data.select_dtypes(exclude=['object'])\n",
        "\n",
        "def generate_interest_labels(num_users, recommendations_df):\n",
        "    interest_labels = {}\n",
        "\n",
        "    for user_idx in range(num_users):\n",
        "        # Get the list of recommended users for the current user\n",
        "        recommended_users = recommendations_df.iloc[user_idx, 1:].values.tolist()  # Get recommended users\n",
        "\n",
        "        # Select the top 1 user from the recommended list (the first user in the list)\n",
        "        top_recommended_user = recommended_users[0]  # The highest-ranked user is the first one\n",
        "\n",
        "        # Assign this top recommended user as the interested user for this user\n",
        "        interest_labels[user_idx] = [top_recommended_user]  # Keep it as a list to maintain consistency\n",
        "\n",
        "    return interest_labels\n",
        "\n",
        "# Generate interest labels for 100 users\n",
        "interest_labels = generate_interest_labels(100, recommendations_df)\n",
        "\n",
        "\n",
        "\n",
        "def construct_double_tower_input(user_data, interest_labels, num_users):\n",
        "    X_user = []\n",
        "    X_recommended = []\n",
        "    y_train = []\n",
        "\n",
        "    # Iterate over all users and their interest labels\n",
        "    for user_idx in range(num_users):\n",
        "        for recommended_idx in range(num_users):\n",
        "            if user_idx != recommended_idx:  # Skip self-pairing\n",
        "                # Append user features and recommended user features\n",
        "                X_user.append(user_data.iloc[user_idx].values)\n",
        "                X_recommended.append(user_data.iloc[recommended_idx].values)\n",
        "\n",
        "                # Label is 1 if the user is interested in the recommended user, 0 otherwise\n",
        "                y_train.append(1 if recommended_idx in interest_labels[user_idx] else 0)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    X_user = np.array(X_user)\n",
        "    X_recommended = np.array(X_recommended)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    return X_user, X_recommended, y_train\n",
        "# Call the function to construct input features\n",
        "X_user, X_recommended, y_train = construct_double_tower_input(df, interest_labels, 100)\n",
        "\n",
        "# Check the shape of the output\n",
        "print(\"X_user shape:\", X_user.shape)\n",
        "print(\"X_recommended shape:\", X_recommended.shape)\n",
        "print(\"y_train shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6luw1xXTge73",
        "outputId": "7b6c52cf-6038-4b9e-e767-d18f64073be3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_user shape: (9900, 11)\n",
            "X_recommended shape: (9900, 11)\n",
            "y_train shape: (9900,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scale training data\n",
        "X_user_unscaled = X_user\n",
        "X_recommended_unscaled = X_recommended\n",
        "\n",
        "# Standard scaling for X_recommended (formerly item_train)\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(X_recommended)\n",
        "X_recommended = scalerItem.transform(X_recommended)\n",
        "\n",
        "# Standard scaling for X_user (formerly user_train)\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(X_user)\n",
        "X_user = scalerUser.transform(X_user)\n",
        "\n",
        "# Now X_user, X_recommended, and y_train are scaled"
      ],
      "metadata": {
        "id": "RH2y-bQ8ynVL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Combine the two inputs into one for oversampling purposes\n",
        "combined_inputs = np.hstack((X_user, X_recommended))\n",
        "\n",
        "# Apply SMOTE to the combined inputs and labels\n",
        "smote = SMOTE()\n",
        "combined_inputs_resampled, y_train_resampled = smote.fit_resample(combined_inputs, y_train)\n",
        "\n",
        "# Split the resampled combined inputs back into X_user and X_recommended\n",
        "X_user = combined_inputs_resampled[:, :X_user.shape[1]]\n",
        "X_recommended = combined_inputs_resampled[:, X_user.shape[1]:]"
      ],
      "metadata": {
        "id": "XEWZpNaBB4nL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj5n4e0fDP_C",
        "outputId": "947ebe1b-cd48-40f4-c775-ffa675d05bcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7859"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train_resampled.copy()\n",
        "# Split X_recommended, X_user, and y_train into training and testing sets (80% train, 20% test)\n",
        "X_recommended_train, X_recommended_test = train_test_split(X_recommended, train_size=0.80, shuffle=True)\n",
        "X_user_train, X_user_test = train_test_split(X_user, train_size=0.80, shuffle=True)\n",
        "y_train, y_test = train_test_split(y_train, train_size=0.80, shuffle=True)\n",
        "\n",
        "# Print the shapes of the training and testing datasets\n",
        "print(f\"Recommended user (item) training data shape: {X_recommended_train.shape}\")\n",
        "print(f\"Recommended user (item) test data shape: {X_recommended_test.shape}\")\n",
        "print(f\"Target user training data shape: {X_user_train.shape}\")\n",
        "print(f\"Target user test data shape: {X_user_test.shape}\")\n",
        "print(f\"y_train training data shape: {y_train.shape}\")\n",
        "print(f\"y_test test data shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjlbMc35zYe1",
        "outputId": "494c64b6-2ac1-4b81-8503-0da165b8aa8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended user (item) training data shape: (15680, 11)\n",
            "Recommended user (item) test data shape: (3920, 11)\n",
            "Target user training data shape: (15680, 11)\n",
            "Target user test data shape: (3920, 11)\n",
            "y_train training data shape: (15680,)\n",
            "y_test test data shape: (3920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Custom layer for L2 normalization\n",
        "class L2NormalizationLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.math.l2_normalize(inputs, axis=1)\n",
        "\n",
        "# Define the user and recommended user tower neural networks\n",
        "num_outputs = 32  # Increased output dimension for concatenation later\n",
        "\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
        "])\n",
        "\n",
        "recommended_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
        "])\n",
        "\n",
        "# Create the user input and connect it to the user tower\n",
        "input_user = tf.keras.layers.Input(shape=(X_user.shape[1],))\n",
        "vu = user_NN(input_user)\n",
        "vu = L2NormalizationLayer()(vu)  # Normalize the output using the custom layer\n",
        "\n",
        "# Create the recommended user input and connect it to the recommended user tower\n",
        "input_recommended = tf.keras.layers.Input(shape=(X_recommended.shape[1],))\n",
        "vr = recommended_NN(input_recommended)\n",
        "vr = L2NormalizationLayer()(vr)  # Normalize the output using the custom layer\n",
        "\n",
        "# Concatenate the two feature vectors from the user and recommended user towers\n",
        "concatenated = tf.keras.layers.Concatenate()([vu, vr])\n",
        "\n",
        "# Add additional dense layers after concatenation to learn more complex relationships\n",
        "dense_combined = tf.keras.layers.Dense(64, activation='relu')(concatenated)\n",
        "dense_combined = tf.keras.layers.Dense(32, activation='relu')(dense_combined)\n",
        "\n",
        "# Output layer with sigmoid activation for binary classification\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense_combined)\n",
        "\n",
        "# Specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_recommended], output)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model with additional evaluation metrics\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "oU4ZNv6ZEtgK",
        "outputId": "014cd0cc-48f6-455c-c07d-a8317bbb6180"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_14\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_18            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_8 (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m46,304\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_9 (\u001b[38;5;33mSequential\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m46,304\u001b[0m │ input_layer_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ l2_normalization_layer_8  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ sequential_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mL2NormalizationLayer\u001b[0m)    │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ l2_normalization_layer_9  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ sequential_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mL2NormalizationLayer\u001b[0m)    │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ l2_normalization_laye… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ l2_normalization_laye… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,160\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m33\u001b[0m │ dense_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_16            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_18            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">46,304</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ sequential_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">46,304</span> │ input_layer_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ l2_normalization_layer_8  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2NormalizationLayer</span>)    │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ l2_normalization_layer_9  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2NormalizationLayer</span>)    │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ l2_normalization_laye… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ l2_normalization_laye… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,881\u001b[0m (386.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,881</span> (386.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m98,881\u001b[0m (386.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,881</span> (386.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function as Mean Squared Error (for regression problems) or binary crossentropy if it's a classification task\n",
        "cost_fn = tf.keras.losses.BinaryCrossentropy()  # Assuming binary classification task\n",
        "\n",
        "# Define the optimizer with a learning rate\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model with the defined optimizer and loss function\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn,\n",
        "              metrics=['accuracy'])  # Add accuracy as a metric\n",
        "\n",
        "# Train the model using the user and recommended user features\n",
        "# Make sure to pass the training data (X_user_train and X_recommended_train) along with the labels y_train\n",
        "history = model.fit([X_user_train, X_recommended_train], y_train, epochs=50, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RYwFaPUK0X2F",
        "outputId": "4d31bd71-9444-4dd8-c7ea-1f07833926aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5018 - loss: 1.2059\n",
            "Epoch 2/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4960 - loss: 1.0593\n",
            "Epoch 3/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5066 - loss: 0.9551\n",
            "Epoch 4/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4991 - loss: 0.8847\n",
            "Epoch 5/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5159 - loss: 0.8343\n",
            "Epoch 6/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5139 - loss: 0.7978\n",
            "Epoch 7/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5158 - loss: 0.7720\n",
            "Epoch 8/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5168 - loss: 0.7529\n",
            "Epoch 9/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5189 - loss: 0.7390\n",
            "Epoch 10/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5168 - loss: 0.7291\n",
            "Epoch 11/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5203 - loss: 0.7216\n",
            "Epoch 12/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5178 - loss: 0.7160\n",
            "Epoch 13/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5245 - loss: 0.7112\n",
            "Epoch 14/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5151 - loss: 0.7081\n",
            "Epoch 15/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5245 - loss: 0.7054\n",
            "Epoch 16/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5173 - loss: 0.7037\n",
            "Epoch 17/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5221 - loss: 0.7017\n",
            "Epoch 18/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5258 - loss: 0.7003\n",
            "Epoch 19/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5143 - loss: 0.6996\n",
            "Epoch 20/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5225 - loss: 0.6981\n",
            "Epoch 21/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5251 - loss: 0.6974\n",
            "Epoch 22/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5225 - loss: 0.6970\n",
            "Epoch 23/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5258 - loss: 0.6962\n",
            "Epoch 24/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5208 - loss: 0.6960\n",
            "Epoch 25/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5232 - loss: 0.6960\n",
            "Epoch 26/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5200 - loss: 0.6955\n",
            "Epoch 27/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5213 - loss: 0.6951\n",
            "Epoch 28/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5260 - loss: 0.6948\n",
            "Epoch 29/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5237 - loss: 0.6952\n",
            "Epoch 30/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5262 - loss: 0.6943\n",
            "Epoch 31/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5259 - loss: 0.6941\n",
            "Epoch 32/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5271 - loss: 0.6936\n",
            "Epoch 33/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5305 - loss: 0.6943\n",
            "Epoch 34/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5294 - loss: 0.6934\n",
            "Epoch 35/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5293 - loss: 0.6939\n",
            "Epoch 36/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5283 - loss: 0.6935\n",
            "Epoch 37/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5264 - loss: 0.6936\n",
            "Epoch 38/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5236 - loss: 0.6940\n",
            "Epoch 39/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5296 - loss: 0.6937\n",
            "Epoch 40/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5223 - loss: 0.6939\n",
            "Epoch 41/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5279 - loss: 0.6934\n",
            "Epoch 42/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5272 - loss: 0.6928\n",
            "Epoch 43/50\n",
            "\u001b[1m490/490\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5269 - loss: 0.6931\n",
            "Epoch 44/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-791994f69699>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train the model using the user and recommended user features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Make sure to pass the training data (X_user_train and X_recommended_train) along with the labels y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_user_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_recommended_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate([X_user_test, X_recommended_test], y_test)\n",
        "\n",
        "# Print the test results\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnNw2OT11Fe9",
        "outputId": "2ad6177f-0289-4de1-d6eb-ff1eb98b4944"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5106 - loss: 0.7857\n",
            "Test Loss: 0.7894521951675415\n",
            "Test Accuracy: 0.5068877339363098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Ttt67b3DwQ",
        "outputId": "e81c3516-4657-44bd-cd1c-45ec8f7de5fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([X_user_test, X_recommended_test])\n",
        "\n",
        "predicted_labels = (predictions > 0.29).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfwaXnlJ3IFB",
        "outputId": "befba333-c97a-40fe-b304-946f36aa9be9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC11HRpu3LTB",
        "outputId": "6d0c5022-e84a-4afb-a4aa-a6f8aead7fcd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[1948    0]\n",
            " [  32    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWlRXiHj4BX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}